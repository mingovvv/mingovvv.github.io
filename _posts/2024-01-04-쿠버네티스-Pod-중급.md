---
title: ☸️ 쿠버네티스 Pod 오브젝트 자세히 살펴보기
author: mingo
date: 2024-01-04 00:30:00 +0900
categories: [Kubernetes, Object]
tags: [pod, kubernetes, k8s]
#image:
#  path: /path/to/image
#  alt: image alternative text
---

-----------------

## Pod LifeCycle
![1.png](/assets/img/post/202401/1.png){: .border .center-image w="600" h="800" }
_Pod와 Container의 라이프사이클_

쿠버네티스 최소배포 단위인 Pod는 하나 이상의 컨테이너로 구성되어 있으며, Pod의 상태는 Pod 내부의 컨테이너 상태에 따라 결정된다. 
Pod의 상태는 크게 4가지로 구분되는데 <kbd>Pending</kbd>, <kbd>Running</kbd>, <kbd>Failed</kbd>, <kbd>Succeeded</kbd> 이 4가지 상태는 Pod의 상태를 나타내는 Phase 필드에 반영된다.

Pod는 Pod의 라이프사이클 중에서 딱 한번만 Scheduled 되어 특정 Node에 할당되며 종료될 때까지 해당 Node에서 실행된다.

![10.png](/assets/img/post/202401/10.png){: .border .center-image w="500" h="400" }
_Pod 상태 조회 - phase 필드_
 - `kubectl get pod <name-of-pod> -o yaml`

- kubedoc - Pod LifeCycle : <https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/>

### 1. Pending Phase
![2.png](/assets/img/post/202401/2.png){: .border .center-image w="800" h="400" }
_Pending Phase_

쿠버네티스 클러스터에서 Pod 생성 요청을 승인하면 Pod의 최초 상태는 `Pending` 상태이며, 본 컨테이너가 기동되기 전에 Volume이나 보안 설정 등 초기화를 위한 `initContainer`가 옵셔널로 실행된다. 
(`initContainer`는 Pod 명세 안에 initContainers 항목으로 초기화 스크립트를 작성할 수 있다.) 
`initContainer`는 본 컨테이너보다 먼저 실행되고 성공적으로 초기화 작업이 종료되면 이어서 생성될 Pod가 어떤 Node에 할당될 지 결정하는 Pod Scheduling 작업이 수행된다. 
Node 결정 후에는 컨테이너의 이미지를 다운로드를 진행한다. (이때 컨테이너 상태는 waiting:ContainerCreating 상태이다.)

### 2. Running Phase
![3.png](/assets/img/post/202401/3.png){: .border .center-image w="800" h="400" }
_Running Phase_
  
본격적으로 컨테이너가 기동되면서 Pod는 Node에 바인딩된다. 
그와 동시에 Pod와 Container의 상태는 `Running` 상태로 변경된다. 
만약 Pod 속 일부 Container에 문제가 발생하면 Container는 재시작되고 Container의 상태는 waiting:CrashLoopBackOff로 변경된다.
하지만 Container가 재시작되고 있는 중에도 Pod는 `Running` 상태를 유지한다.
**(다만 Pod의 내부 컨디션의 `ContainerReady`와 `Ready` 필드는 false 상태가 된다.)**

> Pod 상태가 Running 이여도 내부 Container 상태에 문제가 있을 수 있으므로 내부 컨테이너 상태도 주목할 필요가 있다.
{: .prompt-warning }

Job이나 CronJob으로 생성된 Pod의 경우 자신의 일을 마치게 되면 `Failed` 또는 `Succeeded` 상태로 귀결된다.
(Pod의 ContainerReady와 Ready는 false로 변경된다.)

### 3-1 Failed Phase
작업을 하고 있는 컨테이너 중에 하나라도 문제가 발생하면 Pod의 상태는 Failed가 된다.
또한 `Pending`에서 정상적으로 Pod를 생성하지 못하고 `Failed`로 빠지는 케이스도 있다.

### 3-2 Succeeded Phase
작업을 하고 있는 컨테이너가 모두 작업을 성공적으로 Completed 하였을 경우 Pod의 단계는 `Succeeded`가 된다.

### 3-3 Unknown Phase
`Running` 중에 어떤 이유에 의해서 Pod의 상태를 얻을 수 없게되면 `Unknown`으로 변경된다.
이 단계는 일반적으로 Pod가 실행되어야 하는 Node 통신 오류로 인해 발생한다.

> Pod가 삭제될 때, 일부 kubectl 커맨드에서 `Terminating` 상태로 표시되는 경우가 있다. 이는 Pod가 삭제되기 전에 Pod 내부의 컨테이너가 gracefully한 종료를 위해 기다리는 상태이다. (default : 30s)
{: .prompt-info }

-----------------

## Container State

Pod의 phase뿐 아니라 Pod 내부의 컨테이너 상태를 추적할 수 있고 유용한 정보를 Containers.Status 필드를 통해 제공해준다.
컨테이너의 상태는 <kbd>Waiting</kbd>, <kbd>Running</kbd>, <kbd>Terminated</kbd>가 있다.
컨테이너가 각 상태에 머무르고 있는 이유를 요약하는 Reason 필드를 통해 상세한 정보를 제공해준다.

![11.png](/assets/img/post/202401/11.png){: .border .center-image w="500" h="400" }
_컨테이너 상태 조회_
- `kubectl describe pod <name-of-pod>`

### 1. Waiting State
Running, Terminated 상태가 아닌 경우 컨테이너의 상태는 `Waiting` 상태이다. 
waiting 상태는 컨테이너가 시작되고 이미지를 다운받거나 initContainer를 통해 초기화 작업을 진행하는 등 필요한 모든 작업을 실행하고 있는 중을 의미하는 상태이다.

### 2. Running State
Pod속의 컨테이너가 정상적으로 실행되고 있는 상태이다. 

### 3. Terminated State
컨테이너가 완료될 때까지 실행되었거나 어떠한 이유로 실패되어 종료된 상태이다.

> 컨테이너는 자체 재시작 정책을 default로 가지고 있다. Pod를 생성할 때 재시작 정책을 `spec.restartPolicy ` 필드를 통해 `Always(default)`, `OnFailure`, `Never` 중 하나를 값을 넣어 선택할 수 있다.
{: .prompt-tip } 


-----------------

## Prove

![5.png](/assets/img/post/202401/5.png){: .border .center-image w="800" h="400" }
_Prove의 일련의 과정_

`Prove`는 컨테이너에서 kubelet에 의해 주기적으로 수행하는 상태 검사이다. 
상태 검사를 위해 kubelet은 컨테이너의 프로세스를 실행하고, 프로세스의 상태를 지속적으로 감시한다.

위 스크린샷을 통해 간단하게 이해해보자.

1. Pod와 연결된 Service를 통해 외부에 IP가 노출되고 외부 트래픽이 인입된다. 
2. 스크린샷처럼 두 개의 Pod(Pod1, Pod2)가 존재하는 상황에서는 50%씩 트래픽을 나누어서 처리하게 된다.
3. 그러던 중 Pod2가 할당되어 있던 Node2 서버가 다운이 되면 Pod2는 `Failed` 상태가 되고 트래픽은 Pod1에서 100% 처리한다. 
3. Controller는 Auto-Healing 기능이 있어 다른 Node에 Pod2를 할당하려고 시도할 것이다.   
4. **그때 새로운 Node 위에서 Pod2와 Container는 `Running` 상태가 되고 Service와 연결이 되었지만 App은 아직 구동이 진행중인 순간이 발생한다.**  
5. 서비스와 연결이 되자마자 트래픽의 50%는 Pod2에 유입이 되기 때문에 App이 구동되고 있는 순간에는 유저는 정상적인 응답을 받을 수 없는 상황이 발생한다.

이럴때 필요한 것이 App 구동 순간에 트래픽 실패를 제어하는데 도움을 주는 `Readinessprobe` 기능이다. 
App이 완벽하게 구동되기 전까지는 Service와의 연결을 지연시키는 역할이다.

반면 톰캣같은 웹서버는 정상적으로 기동하지만 애플리케이션이 5xx Error를 리턴하는 비정상 응답의 경우에는 어떻게 해야할까? 
톰캣 서버를 참조하고 있는 Pod는 비정상 상황을 인지하지 못하고 지속적으로 트래픽을 전달할 것이고 지속적인 실패가 반복되는 것은 아닐까?
맞다. 그래서 이런 상활을 대비하기 위해 `Livenessprobe` 기능이 존재한다. 
Livenessprobe는 App에서 문제가 발생하면 Pod를 재시작하여 지속적인 실패를 방지하는 것이 목적이다.

`Readinessprobe`와 `Livenessprobe`는 Pod 생성 시, 추가할 수 있다.

- kubedoc - Prove : <https://kubernetes.io/ko/docs/concepts/workloads/pods/pod-lifecycle/#%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%ED%94%84%EB%A1%9C%EB%B8%8C-probe>
- kubedoc - Readinessprobe & Livenessprobe : <https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>

Pod를 생성할 때, `Prove`를 정의하려면 `PhttpGet`, `Exec`, `tcpSocket` 중 어떤 방식으로 상태 검사를 진행할 지 결정해야 하고 아래 옵션을 통해 상세하게 수치를 조절할 수 있다.

| option field        | desc                             | default |
|:--------------------|----------------------------------|---------|
| initialDelaySeconds | 최초 Probe를 하기 전 delay 시간          | 0초      |
| periodSeconds       | Probe를 확인하는 시간 간격                | 10초     |
| timeoutSeconds      | 결과값을 기다리는 타임아웃 시간                | 1초      |
| successThreshold    | 몇번의 성공 결과를 받아야 최종 성공으로 인지할 것인지   | 1회      |
| failureThreshold    | 몇번의 실패 결과를 받아야 최종 실패로 인지할 것인지    | 3회      |

### Readinessprobe
Container가 요청을 처리할 준비가 되었는지 여부를 나타낸다. `Readinessprobe`가 실패하면 엔드포인트 컨트롤러는 파드에 연관된 모든 서비스들의 엔드포인트에서 파드의 IP주소를 제거한다. 준비성 프로브의 초기 지연 이전의 기본 상태는 Failure 이다. 만약 컨테이너가 준비성 프로브를 지원하지 않는다면, 기본 상태는 Success 이다.

![6.png](/assets/img/post/202401/6.png){: .border w="500" h="500" }
_before_
![7.png](/assets/img/post/202401/7.png){: .border w="500" h="500" }
_after_

```yaml
# Service 생성
apiVersion: v1
kind: Service
metadata:
  name: svc-readiness
spec:
  selector:
    app: readiness
  ports:
  - port: 8080
    targetPort: 8080
```

```yaml
# Pod1 생성 (Service와 연결)
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: readiness  
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
  terminationGracePeriodSeconds: 0
```
- Pod1 트래픽 확인 : `while true; do date && curl <Service-clusterIP>:8080/hostname; sleep 1; done`

```yaml
# Pod2 생성 (Service와 연결, readinessProbe 설정 추가)
apiVersion: v1
kind: Pod
metadata:
  name: pod-readiness-exec1
  labels:
    app: readiness  
spec:
  containers:
  - name: readiness
    image: kubetm/app
    ports:
    - containerPort: 8080
    readinessProbe: # readinessProbe 설정
      exec: # 검증 방식 : EXEC
        command: ["cat", "/readiness/ready.txt"]
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 3
    volumeMounts:
    - name: host-path 
      mountPath: /readiness
  volumes:
  - name : host-path
    hostPath: # Node와 Volume을 연결
      path: /tmp/readiness
      type: DirectoryOrCreate
  terminationGracePeriodSeconds: 0
```
 - ![9.png](/assets/img/post/202401/9.png){: .border w="600" h="200" }
   - 이벤트 확인 : `kubectl get events -w | grep pod-readiness-exec1`
   - pod-readiness-exec1가 생성되었지만 `readinessProbe`로 Pod 검증이 실패하여 Service로 연결되지 않으므로 모든 트래픽은 Pod1에서 처리한다.
 - ![13.png](/assets/img/post/202401/13.png){: .border w="600" h="200" }
   - Pod 컨디션 확인 : `kubectl describe pod pod-readiness-exec1 | grep -A5 Conditions`
   - ready, containersReady 필드가 False이기 때문에 pod-readiness-exec1 Pod는 Service와 연결되지 않는다.
 - ![14.png](/assets/img/post/202401/14.png){: .border w="600" h="200" }
   - Service endpoint 확인 : `kubectl describe endpoints svc-readiness`
 - pod-readiness-exec1가 생성된 Node로 진입하고 /tmp/readiness 경로에 ready.txt 파일을 생성해주면 Volume을 공유하고 있는 pod-readiness-exec1 Pod의 내부 검증(**cat /readiness/ready.txt**)이 성공한다.
 - 이후 Pod 컨디션을 확인하면 ready, containersReady 필드는 모두 True, Service endpoint는 모든 Pod가 할당된 것을 확인할 수 있다.

### Livenessprobe
Container가 동작 중인지 여부를 나타낸다. `Livenessprobe`가 실패하면 kubelet은 컨테이너를 재시작한다.

![8.png](/assets/img/post/202401/8.png){: .border .right w="800" h="800" }

```yaml
# Service 생성
apiVersion: v1
kind: Service
metadata:
  name: svc-liveness
spec:
  selector:
    app: liveness
  ports:
  - port: 8080
    targetPort: 8080
```

```yaml
# Pod1 생성 (Service와 연결)
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    app: liveness
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
  terminationGracePeriodSeconds: 0
```

```yaml
# Pod2 생성 (Service와 연결, livenessProbe 설정 추가)
apiVersion: v1
kind: Pod
metadata:
  name: pod-liveness-httpget1
  labels:
    app: liveness
spec:
  containers:
  - name: liveness
    image: kubetm/app
    ports:
    - containerPort: 8080
    livenessProbe: # livenessProbe 설정
      httpGet: # 검증 방식 : httpGet
        path: /health
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 3
  terminationGracePeriodSeconds: 0
```
 - Pod1, Pos2 트래픽 확인: `while true; do date && curl <Service-clusterIP>:8080/hostname; sleep 1; done`
 - 이벤트 로그 확인(정상) : `watch "kubectl describe pod pod-liveness-httpget1 | grep -A10 Events"`
 - Pod(pod-liveness-httpget1) 응답 변경 -> Http 5xx : `curl <pod-clusterIP>:8080/status500`
 - ![15.png](/assets/img/post/202401/15.png){: .border w="600" h="200" }
   - 이벤트 로그 확인(비정상) : `watch "kubectl describe pod pod-liveness-httpget1 | grep -A10 Events"`
   - Pod 내의 livenessProbe 검증 실패로 인하여 Pod 재시작 확인

-----------------

## QoS
![16.png](/assets/img/post/202401/16.png){: .border w="800" h="450" }
_Quality of Service_

쿠버네티스 클러스터 환경에서 Pod-1이 자원을 더 사용해야 하는 상황이 발생할 때, 할당된 Node의 자원이 부족한 상태에 도달할 수 있다.
이럴 때 쿠버네티스는 동일 Node 내의 다른 Pod-2, Pod-3 등.. 낮은 중요도를 가진 Pod를 다운시켜 자원을 확보하고 Pod-1에게 자원을 제공하는 QoS를 제공해준다. 

**QoS는 Quality of Service의 약자로 Pod의 리소스 요청과 제한을 기반으로 Pod에 대한 우선순위를 결정한다.**
쿠버네티스는 Pod가 생성될 때 **BestEffort**, **Burstable**, **Guaranteed** 3가지의 QoS 클래스 중 하나를 자동으로 적용한다.

- kubedoc - QoS : <https://kubernetes.io/ko/docs/tasks/configure-pod-container/quality-service-pod/>

### 1. BestEffort QOS
`BestEffort QOS`는 컨테이너 리소스가 부족할 때 제거될 가능성이 가장 높은 클래스이다.

`BestEffort QOS` 클래스 생성 조건
 - Pod 내 모든 Container가 memory, cpu requests/limits를 명시하지 않았을 때
 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
    - name: qos-demo-3-ctr
      image: nginx
```

### 2. Burstable QOS
`Burstable QOS`는 리소스가 충분하지 않을 때 일부 리로스를 추가로 사용할 수 있다.

`Burstable QOS` 클래스 생성 조건
 - Pod가 Guaranteed QOS 클래스를 할당받지 못했을 때
 - Pod 내에서 최소한 하나의 컨테이너가 memory 또는 cpu requests/limits 를 명시했을 때

> `Burstable QOS`는 `OOM Score`를 통해 먼저 삭제되는 Pod를 결정한다. 만약 Pod-1가 request.memory 5G이고 App 사용메모리 4G이면 75%의 Usage이다. Pod-2는 request.memory 8G이고 App 사용메모리는 똑같이 4G라면 50%의 Usage라고 할 수 있다. 쿠버네티스는 OOM score가 더 큰 Pod-1를 먼저 제거한다. 
{: .prompt-info }

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
```

### 3. Guaranteed QOS
`Guaranteed QOS`는 시스템의 리소스가 부족해도 리소스를 보장받는 클래스이다.

`Guaranteed QOS`  클래스 생성 조건
 - Pod 내 모든 Container는 spec.containers[].resources.requests, spec.containers[].resources.limits 필드를 통해 memory, cpu limits와 memory, cpu requests 명시해야 한다.
 - Pod 내 모든 Container는 명시한 memory, cpu limitsd와 memory, cpu requests가 동일해야 한다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
```

> Container spec을 명시할 때, memory, cpu limits를 지정하지만 requests를 지정하지 않으면 쿠버네티스는 requests를 limits와 동일하게 설정한다.
{: .prompt-tip }

-----------------

## Node Scheduling
Pod는 기본적으로 쿠버네티스 스케줄링에 따라 생성되는 Node가 결정되지만 사용자에 의해서 특정 Node를 결정하거나 특정 Node를 사용하지 못하도록 관리할 수도 있다.

### Node 선택
#### NodeName
NodeName을 통해 할당되는 Node를 결정하는 방법이다. 
명시적으로 노드명을 선택할 수 있다는 점이 좋아보이지만 실제 클러스터를 운영하다보면 Node 역시 Scale-Out 또는 Scale-In 되어 원하는 노드명이 사용자 의도와 달리 변경될 수 있다.

#### NodeSelector
특정 노드를 지정할 때 권장되는 방식이다. Pod에서 NodeSelector을 통해 선택한 레이블이 달려있는 Node를 지정하여 Pod를 생성할 수 있다. 
**만약 n개의 Node에 같은 레이블이 달려있다면 자원이 더 여유있는 Node로 Pod가 할당된다.**

> NodeSelector를 사용했는데 만약 모든 Node에 매칭이 되는 레이블이 없다면, Pod는 그 어떤 Node에도 할당이 되지 않고 에러가 발생한다.
{: .prompt-danger }

#### Node Affinity
![17.png](/assets/img/post/202401/17.png){: .border w="800" h="450" }
_Node Affinity_

`NodeSelector`를 보완할 수 있는 방식이 `Node Affinity` 방식이다. 
생성할 Pod를 정의할 떄, matchExpressions 표기법을 통해 Key 설정해두면 Node 레이블의 Key와 매칭되는 Node를 선택할 수 있다. (자원이 많은 Node를 선택)  
**만약 해당하는 Key와 매칭되는 Node가 없다면 스케줄러가 판단하고 자원이 많은 Node가 선택되도록 옵션도 줄 수 있다.**

`Node Affinity`는 `required`와 `preferred` 속성을 가지고 있다.  
`required`는 작성한 Key가 Node의 레이블의 Key와 매칭되지 않는다면 Pod가 스케줄링 되지 않도록 하는 필수조건을 의미하는 속성이다.   
`preferred`는 매칭되는 Key를 우선으로 할당하되 Key가 없더라도 Node 자원을 확인하고 적절한 곳에 할당될 수 있도록 한다.
`preferred` 속성에는 weight 속성이 따라오는데 이 속성을 통해 여러 Node가 매칭되었을 때, 매칭되는 Node에게 가중치를 줄 수 있다.

```yaml
# Pod 생성 (nodeAffinity - required)
apiVersion: v1
kind: Pod
metadata:
 name: pod-match-expressions1
spec:
 affinity:
  nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution: # required
    nodeSelectorTerms:
    - matchExpressions:
      -  {key: kr, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

```yaml
# Pod 생성 (nodeAffinity - preferred)
apiVersion: v1
kind: Pod
metadata:
 name: pod-preferred
spec:
 affinity:
  nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution: # preferred
    - weight: 1 # 가중치
      preference:
       matchExpressions:
       - {key: ch, operator: Exists}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

### Pod 집중/분산 할당
#### Pod Affinity
![18.png](/assets/img/post/202401/18.png){: .border w="800" h="450" }  
_Pod Affinity_

Pod가 하나의 Node에 집중되어야 하는 경우 사용하는 속성이다. 
특정 Pod가 어떠한 Node에 할당되어 있다고 가정하였을 때, 또 다른 Pod는 `podAffinity` 속성을 통해 Pod의 래이블이 달려있는 Node를 선택해서 Pod가 같은 Node에 할당될 수 있도록 유도한다.

```yaml
# Pod 생성
apiVersion: v1
kind: Pod
metadata:
 name: web1
 labels:
  type: web1
spec:
 nodeSelector:
  a-team: '1'
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

```yaml
# Pod 생성 (podAffinity를 통해 web1 값을 가지고 있는 Pod가 존재하는 Node에 할당)
apiVersion: v1
kind: Pod
metadata:
 name: server1
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team # a-team 레이블을 가진 Node에 할당
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web1]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

```yaml
# Pod 생성 (podAffinity를 통해 web2 값을 가지고 있는 Pod가 존재하는 Node에 할당)
apiVersion: v1
kind: Pod
metadata:
 name: server2
spec:
 affinity:
  podAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team # a-team 레이블을 가진 Node에 할당
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [web2]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```
 - `web2` 값을 가진 Pod가 없으면 위 Pod는 `Pending` 상태가 지속되다가 `web2` 값을 가진 Pod가 생성되면 그 때 같은 Node 안으로 할당된다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web2
  labels:
     type: web2
spec:
  nodeSelector:
    a-team: '2'
  containers:
  - name: container
    image: kubetm/app
  terminationGracePeriodSeconds: 0
```
 - `web2` 값을 가진 Pod가 생성

#### Anti Affinity
![19.png](/assets/img/post/202401/19.png){: .border w="800" h="450" }  
_Anti Affinity_

Pod가 하나의 Node에 집중되지 않도록 강제하는 방식이다. 
1번 Node에 있는 마스터 Pod가 죽으면 다른 Node에 할당되어 있는 슬레이브 Pod가 백업을 해주어야 하는 경우에 사용된다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: master
  labels:
     type: master
spec:
  nodeSelector:
    a-team: '1'
  containers:
  - name: container
    image: kubetm/app
  terminationGracePeriodSeconds: 0
```

```yaml
# Pod 생성 (podAntiAffinity를 통해 master 값을 가지고 있는 Pod가 존재하는 Node를 피해서 할당)
apiVersion: v1
kind: Pod
metadata:
 name: slave
spec:
 affinity:
  podAntiAffinity: # podAntiAffinity 정의
   requiredDuringSchedulingIgnoredDuringExecution:   
   - topologyKey: a-team
     labelSelector:
      matchExpressions:
      -  {key: type, operator: In, values: [master]}
 containers:
 - name: container
   image: kubetm/app
 terminationGracePeriodSeconds: 0
```

### Node 할당제한
#### Taint / Toleration
![20.png](/assets/img/post/202401/20.png){: .border w="650" h="400" } 
_Taint / Toleration_

Pod가 스케줄링 또는 직접 선택에 의해 선택될 수 없고 Pod의 Toleration 속성 통해야만 해당 Node에 할당될 수 있도록 강제하는 방식이다.

Taint는 3가지 옵션이 있다.
- NoSchedule: 이 효과는 해당 노드에 파드를 스케줄링하지 않도록 막는다.
- PreferNoSchedule: 최대한 해당 노드를 피하려고 시도하지만, 다른 노드에 비해 선호도가 높을 경우에는 스케줄링이 가능하다.
- NoExecute: 이 효과는 이미 해당 노드에 스케줄링된 파드에 대해 특정 조건을 충족하지 않으면 파드를 중지하도록 한다. 예를 들어, 만약 파드가 특정 Taint를 허용하지 않는다면 해당 노드에서 중지된다.

> Pod를 생성할 때, Toleration 속성을 통해 Node를 선택하는 것이 아니라 선택된 Node에 들어간 이후 Node의 Taint와 검증을 할 수 있도록 동작하는 것이다. 즉 Pod는 Toleration 속성을 존재하여도 다른 Node에 할당될 수 있으므로 원하는 Taint Node가 있다면 꼭 nodeSelector를 통해 노드를 지정해주어야 한다. 
{: .prompt-danger }

> 기본적으로 마스터 node에 `NoSchedule` Taint가 적용되어 있기 때문에 마스터 node에 Pod가 할당 되지 않는 것이다.
{: .prompt-info }

> 특정 Node에 장애가 발생하면 쿠버네티스는 자체적으로 noExecute Taint를 장애가 발생한 Node에 적용시키고 noExecute 효과로 인하여 Node의 Pod는 삭제된다. 그 이후 ReplicaSet에 의해 Pod는 문제없는 Node위에서 재할당된다. 
{: .prompt-info }

> 출처
- <https://www.inflearn.com/course/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EA%B8%B0%EC%B4%88/>
- <https://kubernetes.io/ko/docs/concepts/overview/working-with-objects/kubernetes-objects/>
