---
title: ☸️ 쿠버네티스 기본 오브젝트 파악하기
author: mingo
date: 2023-12-20 00:30:00 +0900
categories: [Kubernetes]
tags: [object]
pin: true
---

-----------------

쿠버네티스 오브젝트란 쿠버네티스 시스템에서 영속성을 가진 엔티티를 의미한다. 쿠버네티스 환경에서 실행되는 모든 것은 오브젝트를 사용해서 표현하며 오브젝트를 생성, 수정, 삭제하는 방법을 통해 쿠버네티스 시스템을 제어할 수 있다.
쿠버네티스 오브젝트를 동작시키려면 `kubectl` command line을 사용하거나 쿠버네티스 API를 직접 호출해야 한다.

오브젝트 구성을 확인해보면 기본적으로 `spec`과 `status` 필드가 포함되어 있다.
`spec`은 사용자가 사용할 오브젝트의 특징 및 리소스를 정의하는 필드이고 `status`는 쿠버네티스 시스템에 의해 업데이트 되는 현재 리소스 상태를 나타낸다.
컨트롤 플레인(Master Node)을 통해 전체 클러스터 모든 오브젝트의 실시간 상태를 사용자가 의도한 상태와 일치시키기 위해 능동적으로 관리된다.

사용자는 오브젝트를 생성하기 위해 오브젝트 spec을 정의하고 `JSON 형식`으로 만들어 쿠버네티스 API를 호출하거나 `.yaml` 파일로 만들고 kubectl command line을 통해 생성할 수 있다.

-----------------

## Pod
Pod는 독립적인 서비스를 지닌 컨테이너(Container) 묶음을 의미하는 단위이다.
쿠버네티스의 기본 **배포 단위**이자 최소 하나 이상의 컨테이너를 포함하며 클러스터 내에서 실행되는 배포가 가능한 가장 작은 단위이기도 하다.

> 쿠버네티스 래퍼런스에 의하면 일반적으로 여러 컨테이너를 그룹화하여 하나의 Pod로 구성하는 것은 고급 유스케이스라고 소개한다. 이 패턴은 컨테이너가 밀접하게 결합되어 있고 리소스를 공유해야 하는 특정 인스턴스에 적용하라는 가이드가 있다.
{: .prompt-info }

하나의 Pod에 속한 컨테이너 사이에서는 <kbd>네트워크 인터페이스</kbd>와 <kbd>스토리지</kbd>을 공유한다.
Pod 네트워크 인터페이스를 통해 Pod는 고유한 IP 주소가 할당되고 해당하는 Pod안의 모든 컨테이너는 IP 주소와 포트가 포함된 네트워크 네임스페이스를 공유하는 것이다. 
이러한 Pod 속 가상 네트워크를 통해 내부 컨테이너는 `localhost:port` 를 통해 서로 통신할 수 있는 것이다.

### Container
![33.png](/assets/img/post/202312/33.png){: .border .center-image w="700" h="550" }  
 - 하나의 Pod안에 여러 컨테이너가 존재할 수 있으며 각각의 컨테이너는 Pod 내에서 중복되지 않은 Port와 매핑되어 있어야 한다.
 - Pod 내부는 동일한 가상 네트워크 환경을 공유하고 있으며, 컨테이너 간에 localhost로 통신이 가능하다.
 - Pod가 생성될 때, 고유의 IP 주소가 할당되고 쿠버네티스 클러스터 내부에서만 Pod IP를 통해 Pod 접근이 가능하다.
 - Pod의 IP는 재생성 될 때마다 새롭게 할당받아 휘발성을 지닌 IP다. (Pod의 IP를 직접 사용하는 것은 권장하지 않는다.) 

```yaml
# Pod 생성 (다중 컨테이너)
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec: # spec 명세
  containers:
    - name: container1 # 생성할 컨테이너 명
      image: kubetm/p8000 # 생선할 컨테이너 이미지
      ports:
        - containerPort: 8000 # 생성할 컨테이너 포트
    - name: container2
      image: kubetm/p8080
      ports:
        - containerPort: 8080
```

### Label과 Selector
![34.png](/assets/img/post/202312/34.png){: .border .center-image w="700" h="550" }
 - 오브젝트에 부여되는 Key-Value 쌍으로 오브젝트를 식별하고 구성하는데 사용된다. 오브젝트의 Key는 고유해야 하지만 Value는 중복될 수 있다.
 - Pod 뿐만 아니라 모든 오브젝트를 Label 필드를 추가할 수 있다.
 - Label을 통해 오브젝트 메타데이터를 부여하고 Selector를 통해 오브젝트를 선택할 수 있다. 🠒 이를 통해 오브젝트 리소스를 그룹화하고 관리할 수 있다.
 - Labels and Selector : <https://kubernetes.io/ko/docs/concepts/overview/working-with-objects/labels/>

> Label을 이용하면 `loosely coupled fashion` 즉 느슨한 결합이 가능하므로 사용자가 다수의 오브젝트에 대한 별도의 매핑 정보를 저장할 필요가 없다. 
{: .prompt-info }

```yaml
# Pod 생성 (Label 추가)
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    type: web # Label
    lo: dev # Label
spec:
  containers:
    - name: container
      image: kubetm/init
```

```yaml
# Service 생성 (Selector 추가)
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    type: web # Label
  ports:
  - port: 8080
```

### Node Schedule
![35.png](/assets/img/post/202312/35.png){: .border .center-image w="700" h="550" }
 - 쿠버네티스에서 Node에 Pod를 배치하는 방법은 사용자가 `직접 Node를 선택`하는 방법과 `스케줄러가 판단`하여 Node를 선택하는 방법이 있다.  
 - scheduling : <https://kubernetes.io/ko/docs/concepts/scheduling-eviction/>

1. 사용자가 Pod가 생성될 노드를 직접 선택하는 방법
```yaml
# Pod 생성 (nodeSelector를 통한 node 선택)
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
 nodeSelector: 
  hostname: node-1 # 노드 node-1에 생성할 것을 명시
 containers:
 - name: container1
   image: tmkube/p8000
```

2. 스케줄러가 판단하여 노드를 선택하는 방법
 - Pod를 생성할 때, 스케줄링을 요청하는 API를 호출하고 클러스트는 어떤 노드에 파드를 배치할 지 결정한다. 🠒 스케줄러는 Pod의 요구사항과 클러스터의 status를 고려하여 적절한 노드를 선택한다.
 - 노드의 메모리와 **reources.requests.memory**, **reources.limits.memory** 등 여러가지 스케줄링 옵션이 존재한다. 
 - 사용 메모리가 limits를 초과하면 Pod는 shutdown되고 사용 CPU가 limit를 초과하면 requests 수치까지 낮춘다.  

```yaml
# Pod 생성 (스케줄러 판단을 통한 node 선택)
apiVersion: v1
kind: Pod
metadata:
 name: pod-4
spec:
 containers:
 - name: container1
   image: tmkube/p8000
   reources:
    requests: 
      memory: 2Gi # 초기 요구 리소스
    limits:
      memory: 3Gi # 최대 허용 리소스
```

-----------------

## Service
쿠버네티스 오브젝트인 Service는 동일한 서비스(애플리케이션, 리소스, 마이크로서비스 등)를 제공하는 Pod의 논리적 집합과 그것들에 접근할 수 있는 정책을 정의하는 추상적인 개념이다. 
Service를 통해 Pod에 접근할 수 있는 진입점(IP)을 생성하고 Pod의 상태를 모니터링할 수 있다.
여러 Pod간의 통신을 쉽게 관리할 수 있도록 도와주고 클러스터 내부 및 외부에 애플리케이션 네트워크를 노출시켜 접근할 수 있는 방법을 제공한다.

Service는 일반적으로 Selector를 통해 Pos에 대한 접근을 추상화하지만 Selector 대신 [매칭되는 엔드포인트 슬라이스 오브젝트](https://kubernetes.io/ko/docs/concepts/services-networking/service/#%EC%85%80%EB%A0%89%ED%84%B0%EA%B0%80-%EC%97%86%EB%8A%94-%EC%84%9C%EB%B9%84%EC%8A%A4)를 통해 다른 종류로 추상화할 수도 있다.


### ClusterIP Type
![36.png](/assets/img/post/202312/36.png){: .border .center-image w="700" h="550" }  
 - Selector를 필드를 통해 Pod와 연결된 Service를 생성하면 Service에 적용된 ClusterIP가 생성되며 ClusterIP를 통해 Service와 연결된 Pod에 접근할 수 있다. 
 - `ClusterIP Type`은 쿠버네티스 클러스터 내부에서만 접근이 가능하여 내부 마이크로서비스 또는 운영자와 같은 인가된 사용자가 사용하기 적합하다. (쿠버네티스 대시보드 관리, Pod 상태 디버깅)

> Pod 또한 클러스터 내부에서 직접 접근할 수 있는 IP가 존재하지만 Pod의 IP는 Pod가 이슈가 발생하면 재생성 되기때문에 그에 따라 IP도 새롭게 할당받는다. 이렇듯 휘발성 특징이 있는 IP보다는 Service의 IP를 사용하는 편이 애플리케이션 접근에 대한 신뢰성을 확보하는 방법이다.
{: .prompt-tip }

```yaml
# Service 생성 (ClusterIP Type)
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    app: pod # Label 선택
  ports:
  - port: 9000 # Service 진입 포트
    targetPort: 8080 # Pod 연결 포트
  type: ClusterIP # Service Type 정의 (default ClusterIP)
```

```yaml
# Pod 생성 (Label 정의)
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
     app: pod # Label 정의
spec:
  nodeSelector:
    kubernetes.io/hostname: k8s-node1
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
```

### NodePort Type
![37.png](/assets/img/post/202312/37.png){: .border .center-image w="700" h="550" }
 - type을 `NodePort`로 설정하면 컨트롤 플레인에 의하여 `--service-node-port` 플래그로 지정된 범위에서 랜덤 Port가 모든 **Node에 할당된다.** (default 30000~32767)
 - NodePort는 기본적으로 ClusterIP 특징을 따라가며 역시 ClusterIP는 할당받는다.
 - 쿠버네티스 클러스터에 연결되어 있는 모든 Node에게 똑같은 연결 Port가 생성되어 할당이 되고 외부에서 어느 노드이던 간에 `노드IP:NodePort`로 연결을 하면 NodePort를 적용한 서비스와 그 내부 Pod로 연결되는 것을 보장한다. (**Pod가 상주해있는 Node뿐만 아니라 모든 Node에 같은 Port를 할당시키는 것**)
 - 물리적인 Host Node IP를 통해서 Pod에 접근할 수 있지만 보통 Host Node IP를 내부망에서만 접근할 수 있도록 구성하기 때문에 한계가 있어서 대체로 NodePort Type을 사용한다.
 - 주로 내부망 연결용으로 사용하거나 데모 또는 외부 임시 연결용으로 사용된다.

```yaml
# Service 생성 (NodePort Type)
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
    nodePort: 30000 # NodePort 지정 (30000~32767)
  type: NodePort # NodePort 타입의 서비스임을 명시 
  externalTrafficPolicy: Local
```

> `externalTrafficPolicy` 필드를 `Local`로 설정하면 요청한 Node에 매핑된 Pod에게 트래픽을 전달한다. NodePort Type은 Node로 부터 받은 트래픽을 연결되어 있는 Service 하위 Pod에게 랜덤 분배하는 것이 기본설정이다.  **만약 해당 옵션을 활성화 하였으나 노드에 매핑된 Pod가 없다면 액세스 자체가 안되니 주의할 필요가 있다.** 
{: .prompt-tip }

### Load Balancer Type
![38.png](/assets/img/post/202312/38.png){: .border .center-image w="700" h="550" }
 - type을 `LoadBalancer`로 설정하면 클라우드 공급자의 로드 밸런서를 사용하여 Service에 대한 로드 밸런서를 프로비저닝한다.
 - 기본적으로 NodePort 특징을 따르며 Load Balancer를 통하여 트래픽 분산한다.
 - Load Balancer 접근을 위한 외부접속 IP는 GCP, AWS, Azure 등의 플러그인을 통해 설정해야 한다.
 - 외부 시스템 노출용

> Azure 에서 사용자 지정 공개(public) 유형 loadBalancerIP를 사용하려면, 먼저 정적 유형 공개 IP 주소 리소스를 생성해야 한다. 이 공개 IP 주소 리소스는 클러스터에서 자동으로 생성된 다른 리소스와 동일한 리소스 그룹에 있어야 한다. 예를 들면, MC_myResourceGroup_myAKSCluster_eastus이다. 할당된 IP 주소를 loadBalancerIP로 지정한다. 클라우드 공급자 구성 파일에서 securityGroupName을 업데이트했는지 확인한다. CreatingLoadBalancerFailed 권한 문제 해결에 대한 자세한 내용은 Azure Kubernetes Service (AKS) 로드 밸런서에서 고정 IP 주소 사용 또는 고급 네트워킹 AKS 클러스터에서 CreateLoadBalancerFailed를 참고한다.
{: .prompt-info }
 - <https://learn.microsoft.com/en-us/azure/aks/static-ip>
 - <https://github.com/Azure/AKS/issues/357>


```yaml
# Service 생성 (Load Balancer Type)
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
  type: LoadBalancer # LoadBalancer 타입의 서비스임을 명시
```

-----------------

## Volume
쿠버네티스에서 Volume은 Pod의 컨테이너가 사용하는 추상화된 디스크 또는 디렉토리를 의미한다. 
일반적으로 Pod의 컨테이너는 각각 독립적인 파일 시스템을 가지고 있기 때문에 컨테이너 간에 파일을 공유할 수 없고 이를 해결하기 위해 Volume을 사용한다.
`spec.containers.volumeMounts` 필드를 통해 컨테이너 마운트 디렉토리 경로를 설정하고 `spec.volumes` 필드를 통해 Pod 내부에 생성될 Volume 경로와 볼륨 유형을 설정한다.

 - kubedoc - storage : <https://kubernetes.io/ko/docs/concepts/storage/>

### emptyDir Volume 유형
![8.png](/assets/img/post/202312/8.png){: .border .center-image w="700" h="550" }
 - `emptyDir Volume`은 Pod 내부에 마운트되는 Volume이다. 이것을 통해 컨테이너 간 volume을 공유하는 방식이고 Pod의 라이프사이클과 함께하는 임시 디렉토리이다.

> emptyDir Volume 유형은 Pod 내부에 생성되기 때문에 Pod 장애 발생 시, 해당 Volume은 사라져 일시적인 사용 목적을 가진 대상만 취급하는 것이 좋다.
{: .prompt-danger }


```yaml
# Pod 생성 (emptyDir Volume 유형)
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-1
spec:
  containers:
    - name: container1
      image: kubetm/init
      volumeMounts:
        - name: empty-dir # Pod의 Volume 명
          mountPath: /mount1 # container1 컨테이너 내부의 마운트 경로
    - name: container2
      image: kubetm/init
      volumeMounts:
        - name: empty-dir # Pod의 Volume 명
          mountPath: /mount2 # container2 컨테이너 내부의 마운트 경로
  volumes:
    - name : empty-dir # Pod의 Volume 명
      emptyDir: {} # Pod의 Volume 유형
```

### hostPath Volume 유형
![9.png](/assets/img/post/202312/9.png){: .border .center-image w="700" h="550" }
- `hostPath Volume`은 Node 내부에 마운트되는 Volume이다. Host Node의 Volume에 직접 엑세스하여 관리하는 방식이다.
 - Pod → Node 마운트를 하기 때문에 Pod가 장애가 발생해도 Node의 Volume은 사라지지 않는 영속성을 지니고 있다.
 - 주로 Pod가 **자신이 할당되어 있는 Host Node의 데이터를 읽거나 쓸 때** 사용된다. Pod의 데이터를 Volume에 저장하기 보단 Node에 있는 메타 데이터를 Pod에서 사용하기 위해 사용되곤 한다.

> Volume에 영속적인 특성이 있다고 항상 정상 프로세스가 수행되는 것은 아니다. 예를 들어 어떠한 이유로 Pod가 재생성 될 때 다른 Node에 생성될 수 있으며 다른 Node에 생성된 Pod는 기존에 사용하던 Volume에 접근할 수 없다. (Node간 마운트를 통해 어느정도 해결이 가능하지만 관리 포인트가 늘어나는 셈이다.)
{: .prompt-danger }

> 공식 reference에 따르면 HostPath 볼륨에는 많은 보안 위험이 있으며, 가능하면 HostPath를 사용하지 않는 것이 좋다고 한다. HostPath 볼륨을 사용해야 하는 경우, 필요한 파일 또는 디렉터리로만 범위를 지정하고 ReadOnly로 마운트해야 한다.
{: .prompt-danger }

```yaml
# Pod 생성 (host-path Volume Volume 유형)
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  nodeSelector:
    kubernetes.io/hostname: k8s-node1
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: host-path # 마운트할 Volume 명
      mountPath: /mount1 # container 내부의 마운트 경로
  volumes:
  - name : host-path # 마운트할 Volume 명
    hostPath:
      path: /node-v # Node에 생성되는 마운트 Volume 명
      type: DirectoryOrCreate # 경로에 폴더가 없으면 생성
```

### PVC / PV
![10.png](/assets/img/post/202312/10.png){: .border .center-image w="700" h="550" }  
 - PersistentVolumeClaim / PersistentVolume
 - Pod에게 영속성 있는 Volume을 제공하기 위해 사용한다. (엑세스 흐름 : Pod → PVC → PV → Volume)
 - `PV(PersistentVolume)`는 ADMIN이 Volume의 속성을 정의하고 프로비저닝하거나 스토리지 클래스를 사용하여 동적으로 프로비저닝한 클러스터의 스토리지를 의미한다. 이것은 볼륨 플러그인이지만 Pod와는 별개의 라이프사이클을 가지고 있다.
 - `PVC(PersistentVolumeClaim)`는 USER의 스토리지에 대한 요청을 의미한다. 정의된 PV를 PVC와 바인딩하여 사용한다. 
 - **NFS, GlusterFS, AWS EBS, Azure Disk 등 다양한 Volume Plugin을 통해 PV를 생성할 수 있다.**
 
```yaml
# PersistentVolume 생성
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-03
spec:
  capacity:
    storage: 2G
  accessModes:
    - ReadWriteOnce
  local: # host-path와 유사함
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - {key: kubernetes.io/hostname, operator: In, values: [vm-k8s-node-1]} # node-1에 생성
```

```yaml
# PersistentVolumeClaim 생성
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-01
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1G
  storageClassName: ""
```

```yaml
# Pod 생성 (PVC-PV Volume)
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: pvc-pv
      mountPath: /mount3
  volumes:
  - name : pvc-pv
    persistentVolumeClaim: # 사용할 PVC 선택
      claimName: pvc-01
```

![12.png](/assets/img/post/202312/12.png){: .border .center-image w="700" h="550" }  
![13.png](/assets/img/post/202312/13.png){: .border .center-image w="700" h="550" }  
 - **한번 바인딩된 PV는 다른 클레임(PVC)에서 사용할 수 없다.**
 - PV ↔ PVC의 연결은 `accessModes`와 `storage`를 매핑하여 바인딩된다. 
   - PV storage > PVC storage가 큰 경우, PV의 storage로 강제 바인딩
   - PV storage < PVC storage가 큰 경우, 바인딩이 실패하고 Pending 상태 유지

-----------------

## ConfigMap, Secret
`ConfigMap`은 Key-Value 쌍으로 기밀이 아닌 데이터를 저장하는데 사용되는 오브젝트이고 `Secret`은 암호, 토큰 또는 키와 같은 소량의 기밀 데이터를 저장하는 오브젝트이다.
두 오브젝트는 애플리케이션 코드와 별도로 시스템 환경 데이터를 설정하고자 할 때 사용된다. 예를 들어, 개발환경, 운영환경에서 다르게 사용되는 상수를 제공하여 동일한 이미지의 컨테이너 애플리케이션을 동작시켜도 환경에 따라 다른 설정 값을 적용하여 사용할 수 있다. 
사용 방법으로는 Pod 생성 시, ConfigMap과 Secret 오브젝트를 연결할 수 있고 컨테이너 환경변수로 ConfigMap, Secret 오브젝트 데이터를 넘겨 적용시킬 수 있다. 

> ConfigMap, Secret은 많은 양의 데이터를 보유하도록 설계되지 않았으며 저장된 데이터는 1MiB를 초과할 수 없다. 이 제한보다 큰 설정을 저장해야 하는 경우, 볼륨을 마운트하는 것을 고려하거나 별도의 데이터베이스 또는 파일 서비스를 사용할 수 있다.
{: .prompt-info }

> ConfigMap과 Secret을 Pod에서 적용하기 위해서는 오브젝트들은 동일한 Namespace에 생성되어야 한다.
{: .prompt-info }


- kubedoc - ConfigMap : <https://kubernetes.io/docs/concepts/configuration/configmap/>
- kubedoc - Secret : <https://kubernetes.io/docs/concepts/configuration/secret/>

### Env Literal 형식
![18.png](/assets/img/post/202312/18.png){: .border .center-image w="700" h="550" }
 - key:value로 구성된 상수
 - Pod 생성 시, 컨테이너 안의 환경변수에 세팅하는 방식
 - `Secret`의 경우 value는 Base64로 인코딩되어야 하며, 실제 컨테이너로 `Secret` value가 주입될 때 디코딩된 value가 주입된다.

```yaml
# ConfigMap 생성
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-dev # 생성할 Configmap 명
data:
  SSH: 'false' # 기본값은 String 타입이고 boolean의 경우 quotation이 필요함
  User: dev
```

```yaml
# Secret 생성
apiVersion: v1
kind: Secret
metadata:
  name: sec-dev # 생성할 Secret 명
data:
  Key: MTIzNA== # base64 인코딩 값
```

```yaml
# Pod 생성 (ConfigMap, Secret 연결)
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/init
    envFrom:
    - configMapRef: # ConfigMap Reference
        name: cm-dev # 사용할 Configmap 명
    - secretRef: # Secret Reference
        name: sec-dev # 사용할 Secret 명
```

### Env File 형식
![19.png](/assets/img/post/202312/19.png){: .border .center-image w="700" h="550" }
 - 파일 형식의 ConfigMap과 Secret은 데이터 볼륨으로 마운트할 수 있다.
 - `file명`이 key가 되고 `file내용`이 value가 된다.
 - file 형식의 configmap 생성 : `kubectl create configmap cm-file --from-file=./file.txt`
 - file 형식의 secret 생성 : `kubectl create secret sec-file --from-file=./file.txt`
   - `secret`의 경우 파일 안의 내용은 자동으로 base64로 인코딩되기 때문에 인코딩된 값을 넣을 필요가 없다.

```yaml
# Pod 생성 (ConfigMap, Secret File 형식)
apiVersion: v1
kind: Pod
metadata:
  name: pod-file
spec:
  containers:
  - name: container
    image: kubetm/init
    env:
    - name: file-c
      valueFrom:
        configMapKeyRef:
          name: cm-file
          key: file-c.txt
    - name: file-s
      valueFrom:
        secretKeyRef:
          name: sec-file
          key: file-s.txt
```

### Volume Mount File
![20.png](/assets/img/post/202312/20.png){: .border .center-image w="700" h="550" }
 - ConfigMap file, Secret file을 컨테이너 내부에 마운트하여 사용하는 방식
 - **ConfigMap, Secret 오브젝트가 변경이 되면 이미 환경변수로 주입된 `Env File` 방식은 영향이 없지만 `Volume Mount File`은 변경된 부분이 바로 공유가 된다. 이런 특성을 잘 사용하면 유연한 구성을 만들 수 있다.**
   
```yaml
# Pod 생성 (volumeMounts 형식)
apiVersion: v1
kind: Pod
metadata:
  name: pod-mount
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: file-volume
      mountPath: /mount # 마운트할 경로
  volumes:
  - name: file-volume
    configMap:
      name: cm-file # 마운트할 볼륨은 ConfigMap / Secret file을 사용한다.
```

![14.png](/assets/img/post/202312/14.png){: .border .center-image w="500" h="500" }  
 - 인코딩했던 `Secret` 값이 디코딩되어 환경변수로 주입된 것을 확인할 수 있다.

![15.png](/assets/img/post/202312/15.png){: .border .center-image w="500" h="500" }  
![16.png](/assets/img/post/202312/16.png){: .border .center-image w="500" h="500" }  
![17.png](/assets/img/post/202312/17.png){: .border .center-image w="500" h="500" }  
 - key는 `file이름` value는 `file내용`

-----------------

## Namespace, ResourceQuota, LimitRange
![21.png](/assets/img/post/202312/21.png){: .border .center-image w="800" h="550" }   
쿠버네티스에서 `Namespace`는 리소스 그룹 격리 메커니즘을 제공한다. 리소스의 이름은 Namespace 내에서 유일해야 하며, Namespace scope는 네임스페이스 기반 오브젝트인 Deployment, Service, Pod 등에 적용되며 클러스터 범위 scope인 Node, PersistentVolumes 등에는 적용되지 않는다.
Namespace를 여러 개의 팀이나 프로젝트에 걸쳐서 많은 사용자가 사용하는 환경에서 사용하도록 설계되었고 그에 맞춰 한정적인 쿠버네티스 클러스터 자원을 효울적으로 사용할 수 있는 방법을 제공한다.

그것이 `ResourceQuota`와 `LimitRange`이다.

만약 하나의 Namespace에서 클러스터 자원을 모두 소비하면 다른 Pod는 사용할 자원이 없어 시스템 장애로 이어진다.
이런 이슈를 해결하기 위해 Namespace마다 자원을 끌어올 수 있는 한계치를 지정해주는 **ResourceQuota**를 적용할 수 있다. 
  
즉, 설정을 통해 특정 Namespace 자원이 부족해서 추가로 Pod를 생성하지 못할지언정 다른 Namespace에 영향을 끼치도록 하지 않겠다는 설정이다.
  
또한 Pod의 크기가 너무 크면 Pod가 존재하고 있는 Namespace 영역에 다른 Pod가 들어오기 어려운 상활이 발생하기 때문에 
Namespace 내부로 들어오는 Pod의 크기를 제한하는 LimitRange를 설정 할 수도 있다.

> `kube-` 접두사로 시작하는 네임스페이스는 쿠버네티스 시스템용으로 예약되어 있으므로, 사용자는 이러한 네임스페이스를 생성하지 않는다.
{: .prompt-info }

- kubedoc - Namespace : <https://kubernetes.io/ko/docs/concepts/overview/working-with-objects/namespaces/>
- kubedoc - ResourceQuota : <https://kubernetes.io/ko/docs/concepts/policy/resource-quotas/>
- kubedoc - LimitRange : <https://kubernetes.io/ko/docs/concepts/policy/limit-range/>

### Namespace
![22.png](/assets/img/post/202312/22.png){: .border .center-image w="800" h="650" }
 - Namespace를 통해 클러스터 내에서 리소스를 분리하고 격리할 수 있다. 하나의 클러스터 안에서 여러 팀이나 프로젝트가 사용되는 경우 각각의 팀이나 프로젝트에 해당하는 Namespace를 생성하여 분리된 환경을 구성할 수 있다.
 - 하나의 Namespace 내부에서 pod 이름은 중복될 수 없다.
 - A-Namespace의 Service와 B-Namesapce Pod는 Label로 묶을 수 없다. (Node, PV 이 외의 모든 자원은 scope가 다르니 리소스 공유가 안된다.)
 - Namespace를 지우면 내부 모든 자원이 같이 지워진다.
 - network policy를 통해 기본적으로 다른 Namespace의 Pod, Service와 통신이 가능하다.
 - NodePort를 사용하는 Service는 Namespace로 구분할 수 없다.

```yaml
# Namespace 생성
apiVersion: v1
kind: Namespace
metadata:
  name: nm-1
```

```yaml
# Pod 생성
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  namespace: nm-1 # Namespace 지정
  labels:
    app: pod
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
```

### ResourceQuota
![23.png](/assets/img/post/202312/23.png){: .border .center-image w="700" h="550" }
 - Namespace 자원을 제한하고 추적 및 감시하는 데 사용된다.
 - Namespace에서 만들 수 있는 오브젝트 수와 해당 네임스페이스의 리소스가 사용할 수 있는 총 컴퓨트 리소스의 양을 제한할 수 있다.(CPU, memory, storage, pod cnt, service cnt, configMap cnt, PVC cnt...)

> ResourceQuota `cpu` 및 `memory` 리소스의 경우, 해당 네임스페이스의 모든(신규) 파드에 대하여 리소스에 대한 제한을 설정하도록 강제한다. Namespace에서 cpu 또는 memory에 대해 리소스 할당량을 적용하는 경우, 반드시 제출하는 모든 Pod에 대해 해당 리소스에 requests 또는 limits 중 하나를 지정해야 한다. 그렇지 않으면 컨트롤 플레인이 해당 파드에 대한 생성을 거부한다.
{: .prompt-info }

> 항상 Namespace의 ResourceQuota를 지정하기 전에 해당 Namespace에 Pod가 존재하지 않은 상태로 시작해야 한다.
{: .prompt-info }

| 컴퓨트 리소스 쿼터 이름    | 설명                                                       |
|:-----------------|:---------------------------------------------------------|
| limits.cpu       | 터미널이 아닌 상태의 모든 파드에서 CPU 제한의 합은 이 값을 초과할 수 없음.            |
| limits.memory    | 터미널이 아닌 상태의 모든 파드에서 메모리 제한의 합은 이 값을 초과할 수 없음.            |
| requests.cpu     | 터미널이 아닌 상태의 모든 파드에서 CPU 요청의 합은 이 값을 초과할 수 없음.            |
| requests.memory  | 터미널이 아닌 상태의 모든 파드에서 메모리 요청의 합은 이 값을 초과할 수 없음.            |
| hugepages-<size> | 터미널 상태가 아닌 모든 파드에 걸쳐서, 지정된 사이즈의 휴즈 페이지 요청은 이 값을 초과하지 못함. |
| cpu              | cpu 와 같음.                                                |
| memory           | requests.memory 와 같음.                                    |

> 이 외에도 스토리지 리로스 쿼터, 오브젝트 수 리소스 쿼터 등이 있다. <https://kubernetes.io/ko/docs/concepts/policy/resource-quotas/>

```yaml
# ResourceQuota 생성 (컴퓨터 리소스 제한 : memory)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq-1
  namespace: nm-3
spec:
  hard:
    requests.memory: 1Gi
    limits.memory: 1Gi
```
 - 생성 확인 : `kubectl describe resourcequotas -n nm-3`  
 - ResourceQuota 적용 후, Pod에서 리소스 없이 생성하면? 
   - `pods "pod-2" is forbidden: failed quota: rq-1: must specify limits.memory for: container; requests.memory for: container` 에러가 발생한다.  
 - ResourceQuota 적용 후, Pod에서 리소스를 초과하여 생성하면? 
   - `pods "pod-4" is forbidden: exceeded quota: rq-1, requested: limits.memory=858993459200m,requests.memory=858993459200m, used: limits.memory=512Mi,requests.memory=512Mi, limited: limits.memory=1Gi,requests.memory=1Gi` 에러가 발생한다.  
 
```yaml
# ResourceQuota 생성 (오브젝트 수 제한 : pod)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq-2
  namespace: nm-3
spec:
  hard:
    pods: 2
```

### LimitRange
![24.png](/assets/img/post/202312/24.png){: .border .center-image w="700" h="550" }
 - 기본적으로 컨테이너는 쿠버네티스 클러스터에서 무제한 컴퓨팅 리소스로 실행된다. 쿠버네티스 `ResourceQuota`를 사용하여 네임스페이스에 대한 리소스 사용을 제한할 수 있지만, 이는 컨테이너에 대한 리소스 사용을 제한하지 않는다. 이것을 위해 `LimitRange`를 사용할 수 있다.
 - Container 자원을 제한하고 Pod의 안정성을 유지하도록 도와준다. (각 Pod가 해당 Namespace에서 생성될 수 있는지 자원 체크)

| 컴퓨트 리소스 쿼터 이름               | 설명                                       |
|:----------------------------|:-----------------------------------------|
| min.memory                  | pod에서 생성되는 최소 메모리                        |
| max.memory                  | pod에서 생성되는 최대 메모리                        |
| maxLimitRequestRatio.memory | request와 limit의 최대 허용 비율                 |
| defaultRequest.memory       | pod에 메모리 설정이 없을 때 default request memory |
| default.memory              | pod에 메모리 설정이 없을 때 default memory         |

```yaml
# LimitRange 생성
apiVersion: v1
kind: LimitRange
metadata:
  name: lr-1
spec:
  limits:
  - type: Container # 컨테이너 마다 제약을 건다
    min:
      memory: 0.1Gi
    max:
      memory: 0.4Gi
    maxLimitRequestRatio:
      memory: 3
    defaultRequest: # default
      memory: 0.1Gi
    default: # default
      memory: 0.2Gi
```
 - 생성 확인 : `kubectl describe limitranges --namespace=nm-5`

```yaml
# pod 생성
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/app
    resources:
      requests:
        memory: 0.1Gi
      limits:
        memory: 0.5Gi
```
 - `pods "pod-1" is forbidden: [maximum memory usage per Container is 429496729600m, but limit is 512Mi, memory max limit to request ratio per Container is 3, but provided ratio is 5.000000]`
   - LimitRange에 따르면 limits.max.memory 가 0.4Gi이지만 생성 요청한 Pod의 limit.memory는 0.5Gi
   - requests/limits 비율 역시 잘못 설정됨(허용 : 3 / 요청 : 5)

-----------------

## Controller
![27.png](/assets/img/post/202312/27.png){: .border .center-image w="800" h="400" }

쿠버네티스에서 `Controller` 오브젝트는 지속적으로 시스템 상태를 조절하며 원하는 상태로 유지하는 역할을 한다. Controller는 **현재 상태**를 **의도한 상태**로 변경되도록 시스템을 관찰하며 지속적인 변화에 대응한다.

### Replication Controller, ReplicaSet
![28.png](/assets/img/post/202312/28.png){: .border .center-image w="700" h="550" }

`Replication Controller`는 현재 Deprecated된 오브젝트이며 이를 대체하기 위해 등장한 것이 `ReplicaSet`이다.
`Replication Controller`는 Pod와 Controller를 Label과 selector를 사용해서 매핑하고 `ReplicaSet`은 matchExpressions를 통해 유연한 매핑을 수행할 수 있도록 도와준다.

#### Template
 - 새롭게 생성될 Pod의 템플릿을 정의하는 부분이다.
 - Controller와 Pod는 Label(key-value 쌍)을 통해 연결시킬 수 있으며 template을 통해 원하는 Pod를 매핑시킨다. 
 - Pod가 죽거나 이슈가 생기면 controller는 template에 매핑된 Pod를 재생성시켜 **의도한 상태**가 유지되도록 한다.

> Template 특성을 활용해서 애플리케이션 version 업데이트를 진행할 수 있다. Template에 new version의 pod를 저장하고 now version Pod를 죽이면 Controller의 특성으로 인하여 new version Pod가 적용된 컨테이너가 재생성된다.
{: .prompt-tip }

```yaml
# pod 생성
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    type: web
spec:
  containers:
    - name: container
      image: kubetm/app:v1
  terminationGracePeriodSeconds: 0 # Pod 삭제 시, 대기시간 (default : 30s)
```

```yaml
# Replication Controller 생성 (Deprecated)
apiVersion: v1
kind: ReplicationController
metadata:
  name: replication-1
spec:
  replicas: 1 # Pod의 개수 관리
  selector:
    type: web
  # --- template 영역 ---
  template:
    metadata:
      name: pod-1
      labels: 
        type: web # Pod가 새로 만들어질 때 Controller와 매핑되도록 Label 부여
    spec:
      containers:
        - name: container
          image: tmkube/app:v2 # version 변경 가능
  # --- template 영역 ---
```

#### Replicas
 - Controller가 관리하는 Pod의 개수를 관리하는 설정이고 Pod의 개수와 설정 값에 따라 Pod의 Scale Up, Scale Out이 진행된다.

```yaml
# ReplicaSet 생성
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1 # replicas 설정을 통해 Pod는 Scale In 또는 Scale Out
  selector:
    matchLabels: # Pod와 연결을 위한 레이블 선택
      type: web
  template: # Pod가 죽었을 경우, 새롭게 생성될 Pod를 정의하는 template
    metadata:
      name: pod1 # 무시
      labels:
        type: web
    spec:
      containers:
        - name: container
          image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```
> Template을 적용하는 부분에서 Pod명을 정의하는 부분(`spec.template.metadata.name`)은 사실 무시되는 속성이다. Pod명 중복을 허용하지 않는 클러스터 내부 시스템으로 인하여 `ReplicaSet명 + 임의의 문자`의 Pod가 생성된다.   
{: .prompt-tip }


![25.png](/assets/img/post/202312/25.png){: .border .center-image w="700" h="550" }
- `replicas` 를 2로 올리면 template에 작성한 Pod가 Scale Out 되는 것을 확인할 수 있다. 반대로 줄이면 Pod는 제거된다.
- Pod명이 클러스터 내부시스템으로 인하여 `ReplicaSet명 + 임의의 문자`로 생성된 것을 확인할 수 있다.

![26.png](/assets/img/post/202312/26.png){: .border .center-image w="700" h="550" }
- 편집을 통해 업데이트된 이미지를 ReplicaSet에 적용한 뒤 생성되어 있는 Pod를 임의로 shotdown 시키면 ReplicaSet에 의하여 새로운 이미지를 가진 Pod가 바로 재생성되는 것을 확인할 수 있다.
- 기본적으로 ReplicaSet을 삭제하면 연관된 Pod들도 전부 삭제된다. 오직 ReplicaSet만 삭제하기를 원한다면 `cascade` 옵션을 kubectl을 통해서 적용하면 된다.

> `cascade` 옵션을 통해 Deprecated된 `Replication Controller`을 `ReplicaSet`로 마이그레이션 할 수 있다. `--cascade=false`을 통해 Pod는 남겨두고 Replication Controller만 삭제한 뒤 남겨진 Pod와 ReplicaSet를 매핑시키는 방식이다. </br> 
command : `kubectl delete replicationcontrollers replication1 --cascade=false`  
{: .prompt-tip }

> 쿠버네티스 클러스터에 해당하는 Pod가 존재하지 않더라도 Controller의 replicas 설정에 명시된 Pod의 개수대로 Pod는 생성된다. 
{: .prompt-tip }

#### Selector
![29.png](/assets/img/post/202312/29.png){: .border .center-image w="700" h="550" }
 - ReplicaSet에만 있는 기능이다.
 - `matchLabels`는 기존의 Label-Selector과 동일하게 key와 value가 정확하게 일치해야 Contoller와 Pod가 매핑되는 설정이다.
 - `matchExpressions`는 Label과의 매핑을 좀 더 유연하게 사용하여 넓게 매핑시킬 수 있는 설정이다.

```yaml
# ReplicaSet 생성
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector: # 셀렉터 키-값 확인
    matchLabels:
      type: web
      ver: v1
    matchExpressions:
      - {key: type, operator: In, values: [web]}
      - {key: ver, operator: Exists}
  template:
    metadata:
      labels: # 셀렉터 키-값을 모두 포함해야 생성이 가능하다.
        type: web
        ver: v1
        location: dev
    spec:
      containers:
        - name: container
          image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```
 - operation 옵션
   - Exists
     - matchExpressions에 명시된 Key를 레이블로 가지고 있는 Pod를 선택
   - DoesNotExist
     - matchExpressions에 명시된 Key를 레이블로 가지고 있지 않는 Pod를 선택
   - In
     - matchExpressions에 명시된 Key를 기반으로 레이블 Key를 찾고 value 값을 가진 레이블의 value를 가진 pod를 선택
   - NotIn
     - matchExpressions에 명시된 Key를 기반으로 레이블 Key를 찾고 value 값을 가진 레이블의 value를 가진 pod를 선택

> `spec.selector`에 명시된 모든 키-값은 template 레이블에 포함되어 있어야만 정상적으로 ReplicaSet가 생성된다.
{: .prompt-danger }

-----------------

## Deployment
![30.png](/assets/img/post/202312/30.png){: .border .center-image w="700" h="550" }
`Deployment` 오브젝트는 운영중인 서비스를 업데이트 하기위해 재배포가 필요한 상황에서 도움을 주는 컨트롤러 기능이다.
배포 방식으로는 `ReCreate`, `Rolling Update`, `Blue/Green`, `Canary` 방식 4개로 구분된다.

- kubedoc - Deployment : <https://kubernetes.io/ko/docs/concepts/workloads/controllers/deployment/>

### Deployment - ReCreate 방식
![31.png](/assets/img/post/202312/31.png){: .border .center-image w="700" h="550" }
 - V1의 Pod를 동시에 제거하고 V2의 Pod를 동시에 기동하는 방식
 - 배포 시, 자원 사용량은 동일하지만 서비스 Downtime이 존재하기 때문에 서비스 특성을 고려해서 사용해야 한다.
 - Deployment를 생성할 땐 `selector`, `replicas`, `template`를 설정하는데 이것은 `ReplicaSet` 오브젝트를 만들고 설정한 값을 지정하기 위함이다.
 - Deployment의 `template`을 V2로 업데이트를 해주면 해당 `Deployment` 오브젝트와 연결된 `ReplicaSet` 오브젝트의 `replicas 0`으로 변경된다. 그에 따라 V1의 Pod는 Scale In이 되며 서비스 또한 연결 대상이 없기 때문에 Downtime이 발생하는 것이다.
 - Downtime 발생 후, V2의 `template`을 지닌 ReplicaSet 오브젝트가 동작하여, V2 Pod가 생성되고 Service 오브젝트는 생성된 V2 Pod와 Label을 통해 연결된다.

```yaml
# Deployment 생성 (ReCreate 방식)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  selector: # selector
    matchLabels:
      type: app
  replicas: 2 # replicas
  strategy:
    type: Recreate # ReCreate 방식
  revisionHistoryLimit: 1
  template: # template
    metadata:
      labels:
        type: app
    spec:
      containers:
      - name: container
        image: kubetm/app:v1 # V1
      terminationGracePeriodSeconds: 10
```
> `spec.revisionHistoryLimit` 필드는 replicas 값이 0인 Replica 오브젝트를 삭제하지 않고 가지고 있는 갯수를 설정하는 필드이다. replicas 값이 0인 Replica 오브젝트는 이전 버전으로 롤백할 때 사용된다. (default 10)
{: .prompt-info }

> Deployment 오브젝트를 생성하면 명세에 따라 ReplicaSet 오브젝트, Pod까지 전부 생성된다.
{: .prompt-tip }

![39.png](/assets/img/post/202312/39.png){: .border .center-image w="700" h="550" }
_ReplicaSet 오브젝트_
![40.png](/assets/img/post/202312/40.png){: .border .center-image w="700" h="550" }
_Pod 오브젝트_
 - Deployment 오브젝트를 생성하면 ReplicaSet 오브젝트에 `Pod-template-hash` 라는 Key로 Label이 자동으로 추가된 것을 확인할 수 있는데, Deployment에 의해 새롭게 추가된 ReplicaSet은 자신의 Pod를 구별하기 위해서 추가적인 Label과 Selector을 붙여준다. 

> Kubectl을 통해서 롤백하는 방법 </br>
`kubectl rollout history deployment deployment-1` : 롤백 revision history 조회를 통해 버전 확인 </br> 
`kubectl rollout undo deployment deployment-1 --to-revision=2` : 해당하는 revision으로 롤백 진행
{: .prompt-tip }

### Deployment - Rolling Update 방식
![32.png](/assets/img/post/202312/32.png){: .border .center-image w="700" h="550" }
 - V2 Pod로 template를 교체하면서 Rolling Update가 시작된다. replicas 값이 1인 ReplicaSet 오브젝트가 생성되고 V2 Pod는 Label 매핑을 통해 Service 오브젝트에 연결되어 트래픽을 소화한다.
 - V1 Pod를 하나 줄이기 위해 V1 Pod를 가지고 있는 ReplicaSet의 Replicas는 -1 값이 설정된다. **∞ 노드 하나씩 V2 Pod가 트래픽을 받도록 반복 ∞**
 - 배포 시, 새로운 Pod 생성을 위한 추가 자원을 요구하지만 서비스 Downtime이 존재하지 않는다.

```yaml
# Deployment 생성 (Rolling Update 방식)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-2
spec:
  selector:
    matchLabels:
      type: app2
  replicas: 2
  strategy:
    type: RollingUpdate # Rolling Update 방식
  minReadySeconds: 10
  template:
    metadata:
      labels:
        type: app2
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

> `spec.minReadySeconds` 필드는 Rolling Update가 진행될 때 텀을 주는 설정이다.
{: .prompt-info }

### Deployment - Blue/Green 방식
 - Deployment 오브젝트를 사용하는 개념이 아니라 Controller를 통해 블루/그린 배포 방식을 구현할 수 있다.
 - V2 버전의 Pod를 담당하는 Controller를 생성하고 실제 운영되고 있는 Service 오브젝트의 Label을 V1 Controller에서 V2 Controller로 변경시켜 트래픽을 받는 Pod를 전환시키는 방식이다.
 - 배포 시, V1 Pod 수만큼의 V2 Pod가 존재하여야 하므로 자원 요구량은 2배지만 Service 오브젝트 Label만 다시 V1 Controller로 변경하면 롤백이 되는 장점도 있다. 

```yaml
# ReplicaSet 생성 (V1)
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 2
  selector:
    matchLabels:
      ver: v1
  template:
    metadata:
      name: pod1
      labels:
        ver: v1
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

```yaml
# ReplicaSet 생성 (V2)
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica2
spec:
  replicas: 2
  selector:
    matchLabels:
      ver: v2
  template:
    metadata:
      name: pod1
      labels:
        ver: v2
    spec:
      containers:
      - name: container
        image: kubetm/app:v2
      terminationGracePeriodSeconds: 0
```

### Deployment - Canary 방식
 - Deployment 오브젝트를 사용하는 개념이 아니라 Controller를 통해 카나리 배포 방식을 구현할 수 있다.
 - replicas를 1로 설정한 V2 Controller를 생성하고 기존 Service와 매핑되는 Label을 V2 Controller에 적용하고 트래픽의 일부를 받아 테스트를 진행하는 방식이다.

-----------------

## DaemonSet
![41.png](/assets/img/post/202312/41.png){: .border .center-image w="800" h="300" }

`DaemonSet` 오브젝트는 Node의 자원 상태와 상관없이 Pod를 하나씩 제공한다는 특징이 있다. 
Prometheus 같은 Node 성능 모니터링을 위한 APM 서비스 또는 Filebeat와 같은 로그 수집 서비스가 DeamonSet 오브젝트와 어울리는 서비스라고 할 수 있다.

쿠버네티스도 자체적으로 네트워킹 관리를 위해 각각의 노드에 `DaemonSet` 오브젝트를 통해 Proxy 역할을 하는 Pod를 생성하여 사용하고 있다.

![42.png](/assets/img/post/202312/42.png){: .border .center-image w="700" h="550" }
 - 기본적으로 `DaemonSet` 오브젝트는 selector과 template 필드를 통해 모든 Node에 Pod를 생성한다.
 - `DaemonSet` 오브젝트의 `spec.template.nodeSelector` 필드를 통해 노드를 분별하여 Pod 생성을 조절할 수 있다. 

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-2
spec:
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      labels:
        type: app
    spec:
      nodeSelector:
        os: centos
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
```


> **출처 및 참고자료**
- <https://www.inflearn.com/course/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EA%B8%B0%EC%B4%88/>
- <https://kubernetes.io/ko/docs/concepts/overview/working-with-objects/kubernetes-objects/>
